---
layout: default
---

Reading materials will be frequently updated as the course starts.

## General Introduction to Deep Learning
<strong>Readings</strong>  
<ol>
    <li>
    <a href="https://www.deeplearningbook.org"> Deep Learning Book: A Comprehensive Introduction to Deep Learning </a>
    </li>
     <li>
    <a href="https://www.nature.com/articles/nature14539"> An Introductory Article by LeCun, Bengio, and Hinton Published in *Nature*  </a>
    </li>
    <li>
    <a href="https://www.sciencedirect.com/science/article/pii/S0893608014002135"> History and Development of Neural Networks   </a>
    </li>
    <li>
    <a href="https://arxiv.org/abs/1904.05526"> An Overview from the Statistical Perspective </a>
    </li>

</ol>
<strong>Online resources</strong>  
<ol>
    <li>
    <a href="https://www.d2l.ai"> Online Tutorials </a>
    </li>
     <li>
    <a href="https://amturing.acm.org/lectures.cfm"> Videos of Turing Lectures by Geoffrey Hinton and Yann LeCun</a>
    </li>

</ol>

{% comment %}

## Lecture 1
<strong>Readings</strong>  
<ol>
    <li>
    <a href="https://sinews.siam.org/Details-Page/deep-deep-trouble"> Deep Deep Trouble </a>
    </li>
	<li>
    <a href="https://www.tune.com/blog/global-mobile-why-2016-is-the-global-tipping-point-for-the-mobile-economy/"> Why 2016 is The Global Tipping Point... </a>
    </li>
	<li>
    <a href="http://www.digitalistmag.com/digital-economy/2017/07/19/ai-machine-learning-killing-analytics-as-we-know-it-05223779"> Are AI and ML Killing Analyticals... </a>
    </li>
	<li>
    <a href="https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/"> The Dark Secret at The Heart of AI </a>
    </li>
	<li>
    <a href="http://www.independent.co.uk/life-style/gadgets-and-tech/news/ai-robots-artificial-intelligence-racism-sexism-prejudice-bias-language-learn-from-humans-a7683161.html"> AI Robots Learning Racism... </a>
    </li>
	<li>
    <a href="https://www.theguardian.com/technology/2017/aug/10/faceapp-forced-to-pull-racist-filters-digital-blackface"> FaceApp Forced to Pull â€˜Racist' Filters... </a>
    </li>
	<li>
    <a href="http://nypost.com/2017/07/08/were-losing-a-whole-generation-of-young-men-to-video-games/"> Losing a Whole Generation of Young Men to Video Games </a>
    </li>
</ol>

{% endcomment %}  

## Lecture 1
<strong>Readings</strong>  
<ol>
	<li>
    <a href="https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf"> Emergence of Simple-cell Receptive Field Properties </a>
    </li>
    <li>
    <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"> ImageNet Classification with Deep Convolutional Neural Networks (AlexNet) </a>
    </li>
	<li>
    <a href="https://arxiv.org/abs/1409.1556"> Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG) </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1409.4842.pdf"> Going Deeper with Convolutions (GoogLeNet) </a>
    </li>
    	<li>
    <a href="https://arxiv.org/abs/1512.03385"> Deep Residual Learning for Image Recognition (ResNet) </a>
    </li>
    	<li>
    <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer"> Dropout: A Simple Way to Prevent Neural Networks from
Overfitting </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1502.03167.pdf"> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift </a>
    </li>
	<li>
    <a href="https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"> Visualizing and Understanding Convolutional Neural Networks </a>
    </li>
    	<li>
    <a href="https://arxiv.org/abs/1611.03530"> Understanding Deep Learning Requires Rethinking Generalization </a>
    </li>
</ol>
<strong>Blogs</strong>  
<ol>
    <li>
    <a href="https://medium.com/towards-data-science/an-intuitive-guide-to-deep-network-architectures-65fdc477db41"> An Intuitive Guide to Deep Network Architectures </a>
    </li>
	<li>
    <a href="https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba"> Neural Network Architectures </a>
    </li>
</ol>
<strong>Videos</strong>  
<ol>
    <li>
    <a href="https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=99s"> Deep Visualization Toolbox </a>
    </li>
</ol>

## Lecture 2
1. A. Achille and S. Soatto, Emergence of Invariance and Disentanglement in Deep Representations, JMLR 2018, [https://arxiv.org/pdf/1706.01350.pdf](https://arxiv.org/pdf/1706.01350.pdf)
2. A. Achille and S. Soatto, Where is the Information in a Deep Neural Network? [https://arxiv.org/pdf/1905.12213.pdf](https://arxiv.org/pdf/1905.12213.pdf )
3. (optional) A. Achille et al., The Information Complexity of Learning Tasks, their Structure and their Distance [https://arxiv.org/pdf/1904.03292.pdf](https://arxiv.org/pdf/1904.03292.pdf)
4. A. Achille, M. Rovere and S. Soatto, Critical Learning Periods in Deep Neural Networks, ICLR 2019, [https://arxiv.org/pdf/1711.08856.pdf](https://arxiv.org/pdf/1711.08856.pdf)
5. (optional) A. Achille, G. Mbeng an S. Soatto, Dynamics and Reachability of Learning Tasks, [https://arxiv.org/abs/1810.02440](https://arxiv.org/abs/1810.02440)
6. A. Achille et al., Task2Vec, Task Embedding for Meta Learning, ICCV 2019, [https://arxiv.org/pdf/1902.03545.pdf](https://arxiv.org/pdf/1902.03545.pdf)
7. A. Golaktar et al., Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence, NeurIPS 2019, [https://arxiv.org/pdf/1905.13277.pdf](https://arxiv.org/pdf/1905.13277.pdf)

## Lecture 3
<strong>Readings</strong>  
<ol>
    <li>
    <a href="https://arxiv.org/abs/1905.03684"> Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation </a>
    </li>
    <li>
    <a href="https://arxiv.org/abs/1810.05369"> Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel </a>
    </li>
    <li>
    <a href="https://arxiv.org/abs/1711.00501"> Learning One-hidden-layer Neural Networks with Landscape Design </a>
    </li>
</ol>

## Lecture 4
<strong>Readings</strong>  
<ol>
	<li>
    <a href="https://arxiv.org/abs/1711.00165"> Deep Neural Networks as Gaussian Processes </a>
    </li>
	<li>
    <a href="https://arxiv.org/abs/1902.06720"> Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent </a>
    </li>
    <li>
    <a href="https://arxiv.org/abs/1810.05148"> Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes </a>
    </li>
</ol>

## Lecture 5
<strong>Readings</strong>  
<ol>
	<li>
	<a href="https://arxiv.org/abs/1804.06561"> A Mean Field View of the Landscape of Two-Layers Neural Networks </a>
	</li>
	<li>
	<a href="https://arxiv.org/abs/1902.06015"> Mean-Field Theory of Two-Layers Neural Networks: Dimension-Free Bounds and Kernel Limit </a>
	</li>
	<li>
	<a href="https://arxiv.org/abs/1902.02880"> Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks </a>
	</li>
	<li>
	<a href="https://arxiv.org/abs/1805.09545"> On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport </a>
	</li>
	<li>
	<a href="https://papers.nips.cc/paper/2800-convex-neural-networks.pdf"> Convex Neural Networks </a>
	</li>
</ol>

## Lecture 6
<strong>Readings</strong>  
<ol>
    <li>
    <a href="https://arxiv.org/abs/1806.07572"> Neural Tangent Kernel: Convergence and Generalization in Neural Networks </a>
    </li>
    <li>
    <a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf"> Random Features for Large-Scale Kernel Machines </a>
    </li>
    <li>
    <a href="https://arxiv.org/abs/1906.08899"> Limitations of Lazy Training of Two-layers Neural Networks
 </a>
    </li>
</ol>

## Lecture 7
<strong>Readings</strong>  
<ol>
	
<li>
<a href="https://arxiv.org/abs/1706.06083"> Towards Deep Learning Models Resistant to Adversarial Attacks
 </a>
</li>

<li>
<a href="https://arxiv.org/abs/1805.12152">Robustness May Be at Odds with Accuracy
</a>
</li>

<li>
<a href="https://arxiv.org/abs/1312.6199"> Intriguing Properties of Neural Networks
 </a>
</li>

<li>
<a href="https://arxiv.org/abs/1412.6572"> Explaining and Harnessing Adversarial Examples

 </a>
</li>

</ol>


## Lecture 8
<strong>Readings</strong>  
<ol>

<li>
<a href="https://arxiv.org/abs/1412.6614"> In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning </a>
</li>
<li>
<a href="https://arxiv.org/abs/1802.08246"> Characterizing Implicit Bias in Terms of Optimization Geometry </a>
</li> 
<li>
<a href="https://arxiv.org/abs/1710.10345"> The Implicit Bias of Gradient Descent on Separable Data
 </a>
</li> 

</ol>

## Lecture 9
<strong>Readings</strong>  
<ol>
    <li>
    <a href="https://arxiv.org/abs/1312.6120">Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks</a>
    </li>
    <li>
    <a href="https://arxiv.org/abs/1412.6544">Qualitatively Characterizing Neural Network Optimization Problems
</a>
    </li>
</ol>

## Lecture 10
<strong>Readings</strong>  
<ol>
	<li>
    <a href="https://arxiv.org/abs/1811.07062"> The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size </a>
    </li>
	<li>
    <a href="https://arxiv.org/abs/1406.2572"> Identifying and Attacking the Saddle Point Problem in High-dimensional Non-convex Optimization </a>
    </li>
	<li>
    <a href="https://arxiv.org/abs/1706.04454"> Empirical Analysis of the Hessian of Over-Parametrized Neural Networks
 </a>
    </li>
</ol>

## To be discussed and extra
- [Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961) by Silver et al.
- [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Kingma and Welling
- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) by Goodfellow et al.
- [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530) by Zhang et al.
- [Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?](https://arxiv.org/abs/1504.08291) by Giryes et al.
- [Robust Large Margin Deep Neural Networks](https://arxiv.org/abs/1605.08254) by Sokolic et al.
- [Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems](https://arxiv.org/abs/1605.09232) by Giryes et al.
- [Understanding Trainable Sparse Coding via Matrix Factorization](https://arxiv.org/pdf/1609.00285.pdf) by Moreau and Bruna
- [Why are Deep Nets Reversible: A Simple Theory, With Implications for Training](https://arxiv.org/pdf/1511.05653.pdf) by Arora et al.
- [Stable Recovery of the Factors From a Deep Matrix Product and Application to Convolutional Network](https://arxiv.org/abs/1703.08044) by Malgouyres and Landsberg
- [Optimal Approximation with Sparse Deep Neural Networks](https://www.nari.ee.ethz.ch/commth//pubs/files/deep-approx-17.pdf) by Bolcskei et al.
- [Convolutional Rectifier Networks as Generalized Tensor Decompositions](https://arxiv.org/abs/1603.00162) by Cohen and Shashua
- [Emergence of Invariance and Disentanglement in Deep Representations](https://arxiv.org/pdf/1706.01350.pdf) by Achille and Soatto
- [Deep Learning and the Information Bottleneck Principle](https://arxiv.org/pdf/1503.02406.pdf) by Tishby and Zaslavsky

[back](./)
