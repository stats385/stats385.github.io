## On the mean field theory and the tangent kernel theory for neural networks

Deep neural networks trained with stochastic gradient algorithms often achieve near vanishing training error, and generalize well on test data. Such empirical success of optimization and generalization, however, is quite surprising from a theoretical point of view, mainly due to non-convexity and overparameterization of deep neural networks.

In this lecture, I will address the optimization and generalization of deep neural networks using the mean field theory and the tangent kernel theory. I will discuss about the advantages and shortcomings of these two theories. Then I will talk about two interesting phenomena of the generalization error of linearized neural networks: the staircase phenomenon and the double-descent phenomenon. Finally, I will propose challenges and open problems in analyzing deep neural networks.

![Song Mei](/assets/img/SongMei.jpg)  

[Back](./)
