---
layout: default
---

## Lecture 1 -- Deep Learning Challenge. Is There Theory?
<strong>Readings</strong>  
<ol>
    <li>
    <a href="https://sinews.siam.org/Details-Page/deep-deep-trouble"> Deep Deep Trouble </a>
    </li>
	<li>
    <a href="https://www.tune.com/blog/global-mobile-why-2016-is-the-global-tipping-point-for-the-mobile-economy/"> Why 2016 is The Global Tipping Point... </a>
    </li>
	<li>
    <a href="http://www.digitalistmag.com/digital-economy/2017/07/19/ai-machine-learning-killing-analytics-as-we-know-it-05223779"> Are AI and ML Killing Analyticals... </a>
    </li>
	<li>
    <a href="https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/"> The Dark Secret at The Heart of AI </a>
    </li>
	<li>
    <a href="http://www.independent.co.uk/life-style/gadgets-and-tech/news/ai-robots-artificial-intelligence-racism-sexism-prejudice-bias-language-learn-from-humans-a7683161.html"> AI Robots Learning Racism... </a>
    </li>
	<li>
    <a href="https://www.theguardian.com/technology/2017/aug/10/faceapp-forced-to-pull-racist-filters-digital-blackface"> FaceApp Forced to Pull â€˜Racist' Filters... </a>
    </li>
	<li>
    <a href="http://nypost.com/2017/07/08/were-losing-a-whole-generation-of-young-men-to-video-games/"> Losing a Whole Generation of Young Men to Video Games </a>
    </li>
</ol>

## Lecture 2 -- Overview of Deep Learning From a Practical Point of View 
<strong>Readings</strong>  
<ol>
	<li>
    <a href="https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf"> Emergence of simple cell </a>
    </li>
    <li>
    <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"> ImageNet Classification with Deep Convolutional Neural Networks (Alexnet) </a>
    </li>
	<li>
    <a href="https://arxiv.org/abs/1409.1556"> Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG) </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1409.4842.pdf"> Going Deeper with Convolutions (GoogLeNet) </a>
    </li>
	<li>
    <a href="https://arxiv.org/abs/1512.03385"> Deep Residual Learning for Image Recognition (ResNet) </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1502.03167.pdf"> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift </a>
    </li>
	<li>
    <a href="https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"> Visualizing and Understanding Convolutional Neural Networks </a>
    </li>
</ol>
<strong>Blogs</strong>  
<ol>
    <li>
    <a href="https://medium.com/towards-data-science/an-intuitive-guide-to-deep-network-architectures-65fdc477db41"> An Intuitive Guide to Deep Network Architectures </a>
    </li>
	<li>
    <a href="https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba"> Neural Network Architectures </a>
    </li>
</ol>
<strong>Videos</strong>  
<ol>
    <li>
    <a href="https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=99s"> Deep Visualization Toolbox </a>
    </li>
</ol>

## Lecture 3
<strong>Readings</strong>  
<ol>
    <li>
    <a href="https://www.nari.ee.ethz.ch/commth//pubs/files/deep-2016.pdf"> A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction </a>
    </li>
    <li>
    <a href="https://www.nari.ee.ethz.ch/commth//pubs/files/Energy2017.pdf"> Energy Propagation in Deep Convolutional Neural Networks </a>
    </li>
	<li>
	<a href="https://www.nari.ee.ethz.ch/commth//pubs/files/ICML2016.pdf"> Discrete Deep Feature Extraction: A Theory and New Architectures </a>
	</li>
    <li>
    <a href="https://www.nari.ee.ethz.ch/commth//pubs/files/SPIE2017.pdf"> Topology Reduction in Deep Convolutional Feature Extraction Networks </a>
    </li>
</ol>

## Lecture 4
<strong>Readings</strong>  
<ol>
	<li>
    <a href="https://ankitbpatel715.files.wordpress.com/2017/10/nips2016_todl_v1.pdf"> A Probabilistic Framework for Deep Learning </a>
    </li>
	<li>
    <a href="https://ankitbpatel715.files.wordpress.com/2016/12/todl_semi_supervised_learning_drmm_arxiv_v0.pdf"> Semi-Supervised Learning with the Deep Rendering Mixture Model </a>
    </li>
    <li>
    <a href="https://arxiv.org/abs/1504.00641"> A Probabilistic Theory of Deep Learning </a>
    </li>
</ol>

## Lecture 5
<strong>Readings</strong>  
<ol>
	<li>
	<a href="http://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-058v5.pdf"> Why and When Can Deep-but Not Shallow-networks Avoid the Curse of Dimensionality: A Review </a>
	</li>
	<li>
	<a href="https://arxiv.org/abs/1603.00988"> Learning Functions: When is Deep Better Than Shallow </a>
	</li>
</ol>

## Lecture 6
<strong>Readings</strong>  
<ol>
    <li>
    <a href="https://arxiv.org/pdf/1603.00438.pdf"> Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach </a>
    </li>
    <li>
    <a href="https://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf"> Convolutional Kernel Networks </a>
    </li>
    <li>
    <a href="https://papers.nips.cc/paper/4147-kernel-descriptors-for-visual-recognition.pdf"> Kernel Descriptors for Visual Recognition </a>
    </li>
    <li>
    <a href="https://arxiv.org/pdf/1605.06265.pdf"> End-to-End Kernel Learning with Supervised Convolutional Kernel Networks </a>
    </li>
    <li>
    <a href="https://mitpress.mit.edu/books/learning-kernels"> Learning with Kernels </a>
    </li>
    <li>
    <a href="http://www.harchaoui.eu/zaid/publications/hbm_2013_kertest_ieeespm.pdf"> Kernel Based Methods for Hypothesis Testing </a>
    </li>
</ol>

## Lecture 7
<strong>Readings</strong>  
<ol>
	
<li>
<a href="http://www.pennington.ml/papers/geometry_nns.pdf"> Geometry of Neural Network Loss Surfaces via Random Matrix Theory </a>
</li>

<li>
<a href="http://www.pennington.ml/papers/resurrecting_sigmoid.pdf">Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice</a>
</li>

<li>
<a href="http://www.pennington.ml/papers/nonlinear_rmt.pdf"> Nonlinear random matrix theory for deep learning </a>
</li>


</ol>


## Lecture 8
<strong>Readings</strong>  
<ol>

<li>
<a href="http://papers.nips.cc/paper/6111-deep-learning-without-poor-local-minima"> Deep Learning without Poor Local Minima </a>
</li>
<li>
<a href="https://arxiv.org/abs/1611.01540"> Topology and Geometry of Half-Rectified Network Optimization </a>
</li> 
<li>
<a href="https://arxiv.org/abs/1609.01000"> Convexified Convolutional Neural Networks </a>
</li> 
<li>
<a href="https://arxiv.org/pdf/1705.09280.pdf"> Implicit Regularization in Matrix Factorization </a>
</li> 

</ol>

## Lecture 9
<strong>Readings</strong>  
<ol>
    <li>
    <a href="http://redwood.berkeley.edu/bruno/public/papers/Fukushima1980.pdf">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</a>
    </li>
    <li>
    <a href="http://redwood.berkeley.edu/bruno/papers/perception-as-inference.pdf">Perception as an inference problem</a>
    </li>
    <li>
    <a href="http://redwood.berkeley.edu/bruno/papers/jneurosci93.pdf">A Neurobiological Model of Visual Attention and Invariant Pattern Recognition Based on Dynamic Routing of Information</a>
    </li>
</ol>

## Lecture 10
<strong>Readings</strong>  
<ol>
	<li>
    <a href="https://arxiv.org/pdf/1707.06066.pdf"> Working Locally Thinking Globally: Theoretical Guarantees for Convolutional Sparse Coding </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1607.08194.pdf"> Convolutional Neural Networks Analyzed via Convolutional Sparse Coding </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1708.08705.pdf"> Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary Learning </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1705.03239.pdf"> Convolutional Dictionary Learning via Local Processing </a>
    </li>
</ol>

## To be discussed and extra
- [Emergence of simple cell](https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf) by Olshausen and Field
- [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Kingma and Welling
- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) by Goodfellow et al.
- [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530) by Zhang et al.
- [Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?](https://arxiv.org/abs/1504.08291) by Giryes et al.
- [Robust Large Margin Deep Neural Networks](https://arxiv.org/abs/1605.08254) by Sokolic et al.
- [Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems](https://arxiv.org/abs/1605.09232) by Giryes et al.
- [Understanding Trainable Sparse Coding via Matrix Factorization](https://arxiv.org/pdf/1609.00285.pdf) by Moreau and Bruna
- [Why are Deep Nets Reversible: A Simple Theory, With Implications for Training](https://arxiv.org/pdf/1511.05653.pdf) by Arora et al.
- [Stable Recovery of the Factors From a Deep Matrix Product and Application to Convolutional Network](https://arxiv.org/abs/1703.08044) by Malgouyres and Landsberg
- [Optimal Approximation with Sparse Deep Neural Networks](https://www.nari.ee.ethz.ch/commth//pubs/files/deep-approx-17.pdf) by Bolcskei et al.
- [Convolutional Rectifier Networks as Generalized Tensor Decompositions](https://arxiv.org/abs/1603.00162) by Cohen and Shashua
- [Emergence of Invariance and Disentanglement in Deep Representations](https://arxiv.org/pdf/1706.01350.pdf) by Achille and Soatto
- [Deep Learning and the Information Bottleneck Principle](https://arxiv.org/pdf/1503.02406.pdf) by Tishby and Zaslavsky

[back](./)
